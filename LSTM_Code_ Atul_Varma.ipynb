{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION with LSTM Network in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries, modules\n",
    "import pandas as pd # Pandas library for reading '.csv' files as dataframes\n",
    "import numpy as np # Numpy library for creating and modifying arrays.\n",
    "from keras.layers import Dense, SimpleRNN, GRU, LSTM, Embedding ,Dropout,BatchNormalization# Import layers from Keras\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\atul\\\\Desktop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750, 2)\n",
      "Index(['Text', 'Author'], dtype='object')\n",
      "                                                  Text     Author\n",
      "0    I give Pirrip as my father's family name, on t...    Charles\n",
      "1    Ours was the marsh country, down by the river,...    Charles\n",
      "2    You bring me, to-morrow morning early, that fi...    Charles\n",
      "3    The marshes were just a long black horizontal ...    Charles\n",
      "4    My sister, Mrs. Joe, with black hair and eyes,...    Charles\n",
      "5    My sister had a trenchant way of cutting our b...    Charles\n",
      "6    The effort of resolution necessary to the achi...    Charles\n",
      "7    Some medical beast had revived Tar-water in th...    Charles\n",
      "8    Conscience is a dreadful thing when it accuses...    Charles\n",
      "9    As soon as the great black velvet pall outside...    Charles\n",
      "10   The mist was heavier yet when I got out upon t...    Charles\n",
      "11   All this time, I was getting on towards the ri...    Charles\n",
      "12   Why, see now! said he. When a man's alone on t...    Charles\n",
      "13   I indicated in what direction the mist had shr...    Charles\n",
      "14   So, we had our slices served out, as if we wer...    Charles\n",
      "15   My sister, having so much to do, was going to ...    Charles\n",
      "16   Joe and I going to church, therefore, must hav...    Charles\n",
      "17   The time came, without bringing with it any re...    Charles\n",
      "18   We dined on these occasions in the kitchen, an...    Charles\n",
      "19   Among this good company I should have felt mys...    Charles\n",
      "20   But I don't mean in that form, sir, returned M...    Charles\n",
      "21   Joe threw his eye over them, and pronounced th...    Charles\n",
      "22   As I watched them while they all stood cluster...    Charles\n",
      "23   We were joined by no stragglers from the villa...    Charles\n",
      "24   It was of no use asking myself this question n...    Charles\n",
      "25   With my heart thumping like a blacksmith at Jo...    Charles\n",
      "26   It was a run indeed now, and what Joe called, ...    Charles\n",
      "27   As one of the soldiers, who carried a basket i...    Charles\n",
      "28   The two were kept apart, and each walked surro...    Charles\n",
      "29   After an hour or so of this travelling, we cam...    Charles\n",
      "..                                                 ...        ...\n",
      "720  No, no, said he, it is not so bad as that. Fui...  Stevenson\n",
      "721  Thereupon I told him my story from the first, ...  Stevenson\n",
      "722  By this, I saw he must have heard the name all...  Stevenson\n",
      "723  Well, well, said the lawyer, when I had quite ...  Stevenson\n",
      "724  As he thus moralised on my adventures, he look...  Stevenson\n",
      "725  Ay, ay, said the lawyer, that is how it is wit...  Stevenson\n",
      "726  Why, no, sir, not at all, returned the lawyer....  Stevenson\n",
      "727  True, said Mr. Rankeillor. And yet I imagine i...  Stevenson\n",
      "728  The estate is yours beyond a doubt, replied th...  Stevenson\n",
      "729  Dear doctor! cries he, rubbing his brow. Dear ...  Stevenson\n",
      "730  But it was clear my plan had taken hold upon h...  Stevenson\n",
      "731  Why, so it would appear, says he, filling his ...  Stevenson\n",
      "732  Towards the time I had appointed with Alan, we...  Stevenson\n",
      "733  At that, of course, I understood the purpose o...  Stevenson\n",
      "734  As soon as we were past the Hawes (where I rec...  Stevenson\n",
      "735  And that is more than I could look for, Mr. Th...  Stevenson\n",
      "736  Night was quite come when we came in view of t...  Stevenson\n",
      "737  For some time Alan volleyed upon the door, and...  Stevenson\n",
      "738  This change of note disconcerted Ebenezer; he ...  Stevenson\n",
      "739  Why, says Alan, you that are a man of so much ...  Stevenson\n",
      "740  Come, come, Mr. Ebenezer, said the lawyer, you...  Stevenson\n",
      "741  By that time we had the fire lighted, and a bo...  Stevenson\n",
      "742  So the beggar in the ballad had come home; and...  Stevenson\n",
      "743  So far as I was concerned myself, I had come t...  Stevenson\n",
      "744  Mr. Thomson, says he, is one thing, Mr. Thomso...  Stevenson\n",
      "745  This, says he, is to my bankers, the British L...  Stevenson\n",
      "746  Thereupon he took his farewell, and set out wi...  Stevenson\n",
      "747  Alan and I went slowly forward upon our way, h...  Stevenson\n",
      "748  We came the by-way over the hill of Corstorphi...  Stevenson\n",
      "749  It was coming near noon when I passed in by th...  Stevenson\n",
      "\n",
      "[750 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_csv('AuthorsDataset.csv', encoding='latin-1')\n",
    "\n",
    "\n",
    "print(raw_data.shape)\n",
    "print(raw_data.columns)\n",
    "print(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Text     Author\n",
      "352  No doubt for first offenders, and for all offe...    HGWells\n",
      "7    Some medical beast had revived Tar-water in th...    Charles\n",
      "635  This was all wretched for a stranger like myse...  Stevenson\n",
      "56   When I got up to my little room and said my pr...    Charles\n",
      "60   I crossed the staircase landing, and entered t...    Charles\n",
      "2    You bring me, to-morrow morning early, that fi...    Charles\n",
      "191  By this time we had come to the house, where I...    Charles\n",
      "594  The second day I crossed the island to all sid...  Stevenson\n",
      "731  Why, so it would appear, says he, filling his ...  Stevenson\n",
      "271  Individual liberty in a community is not, as m...    HGWells\n",
      "113  My sister was never left alone now; but Joe mo...    Charles\n",
      "510  It was drawing on to sundown when I met a stou...  Stevenson\n",
      "254  There must always be a certain effect of hardn...    HGWells\n",
      "631  Never a word they spoke as they pulled ashore,...  Stevenson\n",
      "73   Meanwhile, councils went on in the kitchen at ...    Charles\n",
      "133  And now, those six days which were to have run...    Charles\n",
      "738  This change of note disconcerted Ebenezer; he ...  Stevenson\n",
      "523  The house of Shaws stood some five full storey...  Stevenson\n",
      "245  As Wemmick and Miss Skiffins sat side by side,...    Charles\n",
      "628  Looking out between the trees, we could see a ...  Stevenson\n",
      "420  The general effect of a Utopian population is ...    HGWells\n",
      "15   My sister, having so much to do, was going to ...    Charles\n",
      "394  A reciprocal restraint on the part of the husb...    HGWells\n",
      "716  The next day it was agreed that Alan should fe...  Stevenson\n",
      "130  By degrees he fell to reposing such great conf...    Charles\n",
      "33   At the time when I stood in the churchyard rea...    Charles\n",
      "743  So far as I was concerned myself, I had come t...  Stevenson\n",
      "373  I should tell him the thing that was becoming ...    HGWells\n",
      "53   She laughed contemptuously, pushed me out, and...    Charles\n",
      "370  These index cards might conceivably be transpa...    HGWells\n",
      "..                                                 ...        ...\n",
      "193  The garden was too overgrown and rank for walk...    Charles\n",
      "349  The work publicly provided would have to be to...    HGWells\n",
      "545  It was a blessed thing indeed to open my eyes ...  Stevenson\n",
      "441  The social theorists of Utopia, my double expl...    HGWells\n",
      "723  Well, well, said the lawyer, when I had quite ...  Stevenson\n",
      "106  However, her temper was greatly improved, and ...    Charles\n",
      "188  After overhearing this dialogue, I should assu...    Charles\n",
      "148  This I would not hear of, so he took the top, ...    Charles\n",
      "28   The two were kept apart, and each walked surro...    Charles\n",
      "506  With that he got upon his feet, took off his h...  Stevenson\n",
      "409  This quadrangle type of building is the preval...    HGWells\n",
      "203  Casting my eyes along the street at a certain ...    Charles\n",
      "657  As soon as the shadow of the night had fallen,...  Stevenson\n",
      "616  He said it was a bad business. It's wonderful,...  Stevenson\n",
      "260  Yet, after all, why should two men be smiled i...    HGWells\n",
      "164  It happened that the other five children were ...    Charles\n",
      "266  That proposition gives one characteristic diff...    HGWells\n",
      "700  At the door of the first house we came to, Ala...  Stevenson\n",
      "549  The cabin-boy Ransome (from whom I had first h...  Stevenson\n",
      "555  The round-house, for which I was bound, and wh...  Stevenson\n",
      "125  Hold that noise, said Mr. Trabb, with the grea...    Charles\n",
      "641  Sometimes we walked, sometimes ran; and as it ...  Stevenson\n",
      "542  I came to myself in darkness, in great pain, b...  Stevenson\n",
      "697  I knew it was my own doing, and no one else's;...  Stevenson\n",
      "380  Let us set aside at once all nonsense of the s...    HGWells\n",
      "265  The language of Utopia will no doubt be one an...    HGWells\n",
      "401  The former alternative leads either to a roman...    HGWells\n",
      "682  To be sure, there might have been a purpose in...  Stevenson\n",
      "443  The Poietic or creative class of mental indivi...    HGWells\n",
      "145  There was an air of toleration or depreciation...    Charles\n",
      "\n",
      "[750 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "data= shuffle(raw_data)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 2)\n",
      "(150, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train,test = train_test_split(data, test_size = 0.2) \n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Text', 'Author'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting unstructured text to structured numeric form\n",
    "This includes:\n",
    "1. Tokenizing\n",
    "2. Converting sequence of words to sequence of word indeces\n",
    "3. Converting varing length sequences to fixed length sequences through padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((600, 100), (150, 100))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(train.Text)\n",
    "\n",
    "train_x = tokenizer.texts_to_sequences(train.Text)\n",
    "train_x = pad_sequences(train_x, maxlen=100)\n",
    "test_x = tokenizer.texts_to_sequences(test.Text)\n",
    "test_x = pad_sequences(test_x, maxlen=100)\n",
    "\n",
    "train_x.shape, test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 101  622    9  408 2806 5610    6   14 5611 3701    4  479    5 2227\n",
      "   15   23    2   25    7  163  155   15   14 5612    2 5613  883   22\n",
      "    1 5614  255   16    8    9   10 1828    3    1  182 5615    3   14\n",
      "  754   94  104    7   10 2228 5616    2  530   17 1358    1  391  378\n",
      "  312 2229   21   23    2 3702   19  183    2  589  180 2230  272  180\n",
      " 2230   11   49    9   35    5  113  667  531    8    7 2807 5617    5\n",
      "  256  590 1208    7   71   80 3703   40    4   27    1 1556    4  462\n",
      "   14 2808]\n"
     ]
    }
   ],
   "source": [
    "print(train_x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical # This convers the labels to one-hot vectors(Dummies)\n",
    "unique_labels = list(data.Author.unique())\n",
    "train_y = np.array([unique_labels.index(i) for i in train.Author])\n",
    "train_y = to_categorical(train_y)\n",
    "test_y_ = np.array([unique_labels.index(i) for i in test.Author])\n",
    "test_y = to_categorical(test_y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K # This 'K' can be used to create user defined functions in keras\n",
    "\n",
    "# Define a custom function in keras to compute recall.\n",
    "# Arguments:\n",
    "# y_true - Actual labels\n",
    "# y_pred - Predicted labels\n",
    "def recall(y_true, y_pred):\n",
    "    TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    PP = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = TP / (PP + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and training an LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building an LSTM model\n",
    "look_back=4 ##here i am giving to 4 words at a time stamp\n",
    "model = Sequential() # Call Sequential to initialize a network\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index), \n",
    "                    input_length=100, \n",
    "                    output_dim=50)) # Add an embedding layer which represents each unique token as a vector\n",
    "model.add(LSTM(7,input_shape=(1, look_back), return_sequences=False)) # Add an LSTM layer\n",
    "model.add(Dense(3, activation='softmax')) # Add an ouput layer. Since classification, 3 nodes for 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 100, 50)           581550    \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 7)                 1624      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 24        \n",
      "=================================================================\n",
      "Total params: 583,198\n",
      "Trainable params: 583,198\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 3)\n",
      "(150, 3)\n",
      "(600, 100)\n",
      "(150, 100)\n"
     ]
    }
   ],
   "source": [
    "print(train_y.shape)\n",
    "print(test_y.shape)\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450 samples, validate on 150 samples\n",
      "Epoch 1/20\n",
      "450/450 [==============================] - 4s 8ms/step - loss: 1.0969 - acc: 0.3933 - val_loss: 1.0948 - val_acc: 0.4000\n",
      "Epoch 2/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 1.0817 - acc: 0.6333 - val_loss: 1.0901 - val_acc: 0.4267\n",
      "Epoch 3/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 1.0617 - acc: 0.8022 - val_loss: 1.0829 - val_acc: 0.4933\n",
      "Epoch 4/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 1.0298 - acc: 0.9022 - val_loss: 1.0704 - val_acc: 0.5667\n",
      "Epoch 5/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.9744 - acc: 0.9422 - val_loss: 1.0474 - val_acc: 0.6333\n",
      "Epoch 6/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.8767 - acc: 0.9356 - val_loss: 0.9885 - val_acc: 0.6133\n",
      "Epoch 7/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.7192 - acc: 0.9178 - val_loss: 0.8611 - val_acc: 0.6400\n",
      "Epoch 8/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.5696 - acc: 0.9333 - val_loss: 0.7590 - val_acc: 0.7200\n",
      "Epoch 9/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.4186 - acc: 0.9956 - val_loss: 0.6890 - val_acc: 0.7467\n",
      "Epoch 10/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.3165 - acc: 0.9933 - val_loss: 0.5812 - val_acc: 0.8200\n",
      "Epoch 11/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.2486 - acc: 1.0000 - val_loss: 0.5566 - val_acc: 0.8000\n",
      "Epoch 12/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.2109 - acc: 0.9956 - val_loss: 0.5229 - val_acc: 0.8267\n",
      "Epoch 13/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.2054 - acc: 0.9844 - val_loss: 0.5557 - val_acc: 0.8067\n",
      "Epoch 14/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.1675 - acc: 0.9956 - val_loss: 0.5559 - val_acc: 0.8067\n",
      "Epoch 15/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.1865 - acc: 0.9778 - val_loss: 0.7561 - val_acc: 0.6667\n",
      "Epoch 16/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.1659 - acc: 0.9867 - val_loss: 0.6589 - val_acc: 0.7267\n",
      "Epoch 17/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.1387 - acc: 0.9978 - val_loss: 0.5940 - val_acc: 0.7800\n",
      "Epoch 18/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.1246 - acc: 1.0000 - val_loss: 0.5869 - val_acc: 0.7733\n",
      "Epoch 19/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.1134 - acc: 1.0000 - val_loss: 0.5793 - val_acc: 0.7933\n",
      "Epoch 20/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.1081 - acc: 0.9978 - val_loss: 0.5661 - val_acc: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25476f1d278>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_x, train_y, epochs=20, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and evaluation on test data\n",
    "1. Check the network output on test data. What do these values represent?\n",
    "2. Predict the class labels on test data\n",
    "2. Evaluate the model on test data\n",
    "\n",
    "Hint: Check model.predict, model.predict_classes, model.evaluate in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450 samples, validate on 150 samples\n",
      "Epoch 1/20\n",
      "450/450 [==============================] - 3s 7ms/step - loss: 0.0147 - acc: 0.9978 - recall: 0.9978 - val_loss: 0.7791 - val_acc: 0.7733 - val_recall: 0.7733\n",
      "Epoch 2/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0112 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.9063 - val_acc: 0.7267 - val_recall: 0.7200\n",
      "Epoch 3/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.0099 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.9393 - val_acc: 0.7200 - val_recall: 0.7067\n",
      "Epoch 4/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.0087 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.9521 - val_acc: 0.7200 - val_recall: 0.7133\n",
      "Epoch 5/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0077 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.9986 - val_acc: 0.7267 - val_recall: 0.7200\n",
      "Epoch 6/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.0069 - acc: 1.0000 - recall: 1.0000 - val_loss: 1.0011 - val_acc: 0.7267 - val_recall: 0.7200\n",
      "Epoch 7/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0062 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.9710 - val_acc: 0.7400 - val_recall: 0.7400\n",
      "Epoch 8/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0056 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.9551 - val_acc: 0.7600 - val_recall: 0.7533\n",
      "Epoch 9/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.0051 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.9480 - val_acc: 0.7600 - val_recall: 0.7533\n",
      "Epoch 10/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.0047 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.9140 - val_acc: 0.7867 - val_recall: 0.7733\n",
      "Epoch 11/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.0043 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.8799 - val_acc: 0.7867 - val_recall: 0.7800\n",
      "Epoch 12/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.0040 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.8943 - val_acc: 0.7733 - val_recall: 0.7667\n",
      "Epoch 13/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0037 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.9317 - val_acc: 0.7600 - val_recall: 0.7533\n",
      "Epoch 14/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.0034 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.9617 - val_acc: 0.7733 - val_recall: 0.7733\n",
      "Epoch 15/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0032 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.9829 - val_acc: 0.7667 - val_recall: 0.7667\n",
      "Epoch 16/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.0030 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.9532 - val_acc: 0.7800 - val_recall: 0.7733\n",
      "Epoch 17/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0155 - acc: 0.9978 - recall: 0.9978 - val_loss: 0.9542 - val_acc: 0.7867 - val_recall: 0.7867\n",
      "Epoch 18/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.0492 - acc: 0.9911 - recall: 0.9911 - val_loss: 0.9736 - val_acc: 0.7733 - val_recall: 0.7733\n",
      "Epoch 19/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.2730 - acc: 0.9400 - recall: 0.9400 - val_loss: 1.2871 - val_acc: 0.6733 - val_recall: 0.6733\n",
      "Epoch 20/20\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.0263 - acc: 0.9911 - recall: 0.9911 - val_loss: 0.7779 - val_acc: 0.7267 - val_recall: 0.7267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25477d377f0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',recall])\n",
    "model.fit(train_x, train_y, epochs=20, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07045849, 0.8476717 , 0.08186986], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prob = model.predict(test_x)\n",
    "test_prob[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 0 2 1 2 2 1 1 1 1 2 0 0 0 1 1 1 1 0 0 0 2 2 1 0 0 1 1 2 1 1 1 2 0 0\n",
      " 0 1 1 0 2 1 1 1 1 0 1 1 0 2 2 2 1 2 2 0 2 1 0 1 0 1 1 1 0 1 1 2 0 1 2 0 1\n",
      " 2 2 2 1 0 2 1 1 0 0 1 1 0 0 2 1 2 2 2 2 0 1 0 1 2 1 1 2 1 1 1 1 1 1 1 0 1\n",
      " 1 1 2 1 2 1 0 0 0 2 0 1 1 2 2 2 1 2 1 1 1 1 1 2 0 2 1 0 1 2 1 1 1 2 2 1 0\n",
      " 2 1]\n"
     ]
    }
   ],
   "source": [
    "test_prob = model.predict(test_x)\n",
    "test_prob[5]\n",
    "test_classes = model.predict_classes(test_x)\n",
    "test_classes.shape\n",
    "print(test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K # This 'K' can be used to create user defined functions in keras\n",
    "\n",
    "# Define a custom function in keras to compute recall.\n",
    "# Arguments:\n",
    "# y_true - Actual labels\n",
    "# y_pred - Predicted labels\n",
    "def recall(y_true, y_pred):\n",
    "    TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    PP = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = TP / (PP + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.recall>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now tuning the privious model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding an intermediate layer in keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building an LSTM model\n",
    "look_back=15##here i am giving to 15 words at a time stamp\n",
    "model = Sequential() # Call Sequential to initialize a network\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index), \n",
    "                    input_length=100, \n",
    "                    output_dim=100)) # Add an embedding layer which represents each unique token as a vector\n",
    "model.add(LSTM(15,input_shape=(1, look_back), return_sequences=False)) # Add an LSTM layer\n",
    "model.add(Dense(3, activation='softmax')) # Add an ouput layer. Since classification, 3 nodes for 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450 samples, validate on 150 samples\n",
      "Epoch 1/20\n",
      "450/450 [==============================] - 4s 8ms/step - loss: 1.0951 - acc: 0.3911 - recall: 0.0000e+00 - val_loss: 1.0902 - val_acc: 0.4733 - val_recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 1.0527 - acc: 0.7422 - recall: 0.0000e+00 - val_loss: 1.0715 - val_acc: 0.4600 - val_recall: 0.0000e+00\n",
      "Epoch 3/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.9636 - acc: 0.7667 - recall: 0.0022 - val_loss: 1.0013 - val_acc: 0.5333 - val_recall: 0.0067\n",
      "Epoch 4/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.7455 - acc: 0.6911 - recall: 0.4756 - val_loss: 0.8435 - val_acc: 0.5867 - val_recall: 0.4400\n",
      "Epoch 5/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.5394 - acc: 0.7356 - recall: 0.6689 - val_loss: 0.6825 - val_acc: 0.6600 - val_recall: 0.5733\n",
      "Epoch 6/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.3697 - acc: 0.9289 - recall: 0.8022 - val_loss: 0.5932 - val_acc: 0.7533 - val_recall: 0.6467\n",
      "Epoch 7/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.2320 - acc: 0.9956 - recall: 0.9956 - val_loss: 0.5645 - val_acc: 0.8067 - val_recall: 0.7733\n",
      "Epoch 8/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.1422 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.5226 - val_acc: 0.8267 - val_recall: 0.8267\n",
      "Epoch 9/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.1134 - acc: 0.9911 - recall: 0.9911 - val_loss: 0.5873 - val_acc: 0.7533 - val_recall: 0.7467\n",
      "Epoch 10/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.1037 - acc: 0.9867 - recall: 0.9867 - val_loss: 0.7567 - val_acc: 0.7333 - val_recall: 0.7133\n",
      "Epoch 11/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0958 - acc: 0.9911 - recall: 0.9911 - val_loss: 0.4369 - val_acc: 0.8533 - val_recall: 0.8467\n",
      "Epoch 12/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0734 - acc: 0.9978 - recall: 0.9978 - val_loss: 0.4392 - val_acc: 0.8467 - val_recall: 0.8333\n",
      "Epoch 13/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0588 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.4165 - val_acc: 0.8600 - val_recall: 0.8467\n",
      "Epoch 14/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0488 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.4326 - val_acc: 0.8400 - val_recall: 0.8400\n",
      "Epoch 15/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0421 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.4450 - val_acc: 0.8400 - val_recall: 0.8400\n",
      "Epoch 16/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0366 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.4609 - val_acc: 0.8333 - val_recall: 0.8333\n",
      "Epoch 17/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0323 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.4827 - val_acc: 0.8267 - val_recall: 0.8267\n",
      "Epoch 18/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0288 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.5317 - val_acc: 0.8000 - val_recall: 0.7933\n",
      "Epoch 19/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0258 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.5925 - val_acc: 0.7733 - val_recall: 0.7667\n",
      "Epoch 20/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0233 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.6169 - val_acc: 0.7600 - val_recall: 0.7600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25477a49630>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',recall])\n",
    "model.fit(train_x, train_y, epochs=20, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##above result is the biasing case then need more tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building an LSTM model\n",
    "look_back=715##here i am giving to 4 words at a time stamp\n",
    "model = Sequential() # Call Sequential to initialize a network\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index), \n",
    "                    input_length=100, \n",
    "                    output_dim=100)) # Add an embedding layer which represents each unique token as a vector\n",
    "model.add(LSTM(8,input_shape=(1, look_back), return_sequences=False)) # Add an LSTM layer\n",
    "model.add(Dense(3, activation='softmax')) # Add an ouput layer. Since classification, 3 nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450 samples, validate on 150 samples\n",
      "Epoch 1/20\n",
      "450/450 [==============================] - 4s 9ms/step - loss: 1.0965 - acc: 0.3978 - recall: 0.0000e+00 - val_loss: 1.0910 - val_acc: 0.4667 - val_recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 1.0704 - acc: 0.6822 - recall: 0.0000e+00 - val_loss: 1.0814 - val_acc: 0.5467 - val_recall: 0.0000e+00\n",
      "Epoch 3/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 1.0299 - acc: 0.8022 - recall: 0.0000e+00 - val_loss: 1.0591 - val_acc: 0.5733 - val_recall: 0.0000e+00\n",
      "Epoch 4/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.9474 - acc: 0.9222 - recall: 0.0000e+00 - val_loss: 1.0053 - val_acc: 0.6200 - val_recall: 0.0000e+00\n",
      "Epoch 5/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.7935 - acc: 0.9600 - recall: 0.1756 - val_loss: 0.8916 - val_acc: 0.6733 - val_recall: 0.1133\n",
      "Epoch 6/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.6054 - acc: 0.9911 - recall: 0.7933 - val_loss: 0.7970 - val_acc: 0.7067 - val_recall: 0.4800\n",
      "Epoch 7/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.4419 - acc: 0.9978 - recall: 0.9933 - val_loss: 0.6506 - val_acc: 0.8200 - val_recall: 0.7400\n",
      "Epoch 8/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.3397 - acc: 0.9956 - recall: 0.9911 - val_loss: 0.5902 - val_acc: 0.8133 - val_recall: 0.8067\n",
      "Epoch 9/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.2743 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.6129 - val_acc: 0.7933 - val_recall: 0.7600\n",
      "Epoch 10/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.2282 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.5715 - val_acc: 0.7933 - val_recall: 0.7867\n",
      "Epoch 11/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.1944 - acc: 0.9978 - recall: 0.9956 - val_loss: 0.6203 - val_acc: 0.7733 - val_recall: 0.7667\n",
      "Epoch 12/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.1589 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.6822 - val_acc: 0.7533 - val_recall: 0.7467\n",
      "Epoch 13/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.1337 - acc: 0.9978 - recall: 0.9978 - val_loss: 0.6645 - val_acc: 0.7667 - val_recall: 0.7600\n",
      "Epoch 14/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.1178 - acc: 0.9956 - recall: 0.9956 - val_loss: 0.7167 - val_acc: 0.7200 - val_recall: 0.7200\n",
      "Epoch 15/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.1078 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.6894 - val_acc: 0.7067 - val_recall: 0.7067\n",
      "Epoch 16/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.1020 - acc: 0.9956 - recall: 0.9956 - val_loss: 0.6636 - val_acc: 0.7400 - val_recall: 0.7333\n",
      "Epoch 17/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0895 - acc: 0.9978 - recall: 0.9978 - val_loss: 0.6172 - val_acc: 0.7667 - val_recall: 0.7533\n",
      "Epoch 18/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0730 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.6029 - val_acc: 0.7733 - val_recall: 0.7667\n",
      "Epoch 19/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0641 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.6169 - val_acc: 0.7667 - val_recall: 0.7667\n",
      "Epoch 20/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0567 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.6145 - val_acc: 0.7733 - val_recall: 0.7733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25408223a90>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',recall])\n",
    "model.fit(train_x, train_y, epochs=20, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now I am using drop outs for over fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building an LSTM model\n",
    "look_back=7##here i am giving to 4 words at a time stamp\n",
    "model = Sequential() # Call Sequential to initialize a network\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index), \n",
    "                    input_length=100, \n",
    "                    output_dim=100)) # Add an embedding layer which represents each unique token as a vector\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(15,input_shape=(1, look_back), return_sequences=False)) # Add an LSTM layer\n",
    "\n",
    "model.add(Dense(3, activation='softmax')) # Add an ouput layer. Since classification, 3 nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450 samples, validate on 150 samples\n",
      "Epoch 1/20\n",
      "450/450 [==============================] - 4s 9ms/step - loss: 1.0973 - acc: 0.3489 - recall: 0.0000e+00 - val_loss: 1.0905 - val_acc: 0.5000 - val_recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 1.0746 - acc: 0.6911 - recall: 0.0000e+00 - val_loss: 1.0802 - val_acc: 0.5933 - val_recall: 0.0000e+00\n",
      "Epoch 3/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 1.0294 - acc: 0.8644 - recall: 0.0000e+00 - val_loss: 1.0382 - val_acc: 0.6467 - val_recall: 0.0000e+00\n",
      "Epoch 4/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.8582 - acc: 0.8911 - recall: 0.1911 - val_loss: 0.8690 - val_acc: 0.7067 - val_recall: 0.2200\n",
      "Epoch 5/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.6455 - acc: 0.9422 - recall: 0.3467 - val_loss: 0.9279 - val_acc: 0.6467 - val_recall: 0.1400\n",
      "Epoch 6/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.5619 - acc: 0.9689 - recall: 0.5622 - val_loss: 0.8002 - val_acc: 0.7733 - val_recall: 0.3200\n",
      "Epoch 7/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.4545 - acc: 0.9889 - recall: 0.8622 - val_loss: 0.7255 - val_acc: 0.7467 - val_recall: 0.5067\n",
      "Epoch 8/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.3350 - acc: 0.9911 - recall: 0.9711 - val_loss: 0.7130 - val_acc: 0.7600 - val_recall: 0.6867\n",
      "Epoch 9/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.2027 - acc: 0.9956 - recall: 0.9933 - val_loss: 0.6895 - val_acc: 0.7600 - val_recall: 0.7400\n",
      "Epoch 10/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.1291 - acc: 0.9956 - recall: 0.9956 - val_loss: 0.4501 - val_acc: 0.8467 - val_recall: 0.8333\n",
      "Epoch 11/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0958 - acc: 0.9978 - recall: 0.9978 - val_loss: 0.5121 - val_acc: 0.8333 - val_recall: 0.8333\n",
      "Epoch 12/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0737 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.5402 - val_acc: 0.8333 - val_recall: 0.8267\n",
      "Epoch 13/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0582 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.6697 - val_acc: 0.8000 - val_recall: 0.8000\n",
      "Epoch 14/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0535 - acc: 0.9978 - recall: 0.9978 - val_loss: 1.0559 - val_acc: 0.7000 - val_recall: 0.7000\n",
      "Epoch 15/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0746 - acc: 0.9911 - recall: 0.9911 - val_loss: 0.4417 - val_acc: 0.8533 - val_recall: 0.8133\n",
      "Epoch 16/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0523 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.4111 - val_acc: 0.8533 - val_recall: 0.8333\n",
      "Epoch 17/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0445 - acc: 1.0000 - recall: 0.9978 - val_loss: 0.3832 - val_acc: 0.8667 - val_recall: 0.8533\n",
      "Epoch 18/20\n",
      "450/450 [==============================] - 2s 3ms/step - loss: 0.0354 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.3891 - val_acc: 0.8533 - val_recall: 0.8467\n",
      "Epoch 19/20\n",
      "450/450 [==============================] - 1s 3ms/step - loss: 0.0300 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.4351 - val_acc: 0.8467 - val_recall: 0.8333\n",
      "Epoch 20/20\n",
      "450/450 [==============================] - 2s 3ms/step - loss: 0.0259 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.5044 - val_acc: 0.8267 - val_recall: 0.8200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2540c5bd8d0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',recall])\n",
    "model.fit(train_x, train_y, epochs=20, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building an LSTM model\n",
    "look_back=7##here i am giving to 4 words at a time stamp\n",
    "model = Sequential() # Call Sequential to initialize a network\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index), \n",
    "                    input_length=100, \n",
    "                    output_dim=150)) # Add an embedding layer which represents each unique token as a vector\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(12,input_shape=(1, look_back), return_sequences=False)) # Add an LSTM layer\n",
    "\n",
    "\n",
    "model.add(Dense(3, activation='softmax')) # Add an ouput layer. Since classification, 3 nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450 samples, validate on 150 samples\n",
      "Epoch 1/20\n",
      "450/450 [==============================] - 5s 12ms/step - loss: 1.0940 - acc: 0.3956 - recall: 0.0000e+00 - val_loss: 1.0869 - val_acc: 0.4600 - val_recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 1.0562 - acc: 0.6444 - recall: 0.0000e+00 - val_loss: 1.0692 - val_acc: 0.4600 - val_recall: 0.0000e+00\n",
      "Epoch 3/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.9897 - acc: 0.7000 - recall: 0.0000e+00 - val_loss: 1.0230 - val_acc: 0.4800 - val_recall: 0.0000e+00\n",
      "Epoch 4/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.8170 - acc: 0.7022 - recall: 0.3222 - val_loss: 0.8864 - val_acc: 0.6000 - val_recall: 0.3800\n",
      "Epoch 5/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.5993 - acc: 0.7933 - recall: 0.6467 - val_loss: 0.7508 - val_acc: 0.6800 - val_recall: 0.5067\n",
      "Epoch 6/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.4874 - acc: 0.8822 - recall: 0.6733 - val_loss: 0.8092 - val_acc: 0.6533 - val_recall: 0.5600\n",
      "Epoch 7/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.4097 - acc: 0.9378 - recall: 0.7933 - val_loss: 0.6889 - val_acc: 0.7467 - val_recall: 0.5533\n",
      "Epoch 8/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.3056 - acc: 0.9822 - recall: 0.9622 - val_loss: 0.6095 - val_acc: 0.7800 - val_recall: 0.7000\n",
      "Epoch 9/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.2181 - acc: 0.9911 - recall: 0.9867 - val_loss: 0.5906 - val_acc: 0.7733 - val_recall: 0.7467\n",
      "Epoch 10/20\n",
      "450/450 [==============================] - 2s 5ms/step - loss: 0.1491 - acc: 0.9978 - recall: 0.9978 - val_loss: 0.5263 - val_acc: 0.8133 - val_recall: 0.7733\n",
      "Epoch 11/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.1015 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.4725 - val_acc: 0.8333 - val_recall: 0.8133\n",
      "Epoch 12/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0731 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.5140 - val_acc: 0.8133 - val_recall: 0.8067\n",
      "Epoch 13/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0597 - acc: 0.9978 - recall: 0.9978 - val_loss: 0.5513 - val_acc: 0.7933 - val_recall: 0.7800\n",
      "Epoch 14/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0550 - acc: 0.9978 - recall: 0.9978 - val_loss: 0.4941 - val_acc: 0.8333 - val_recall: 0.8133\n",
      "Epoch 15/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0503 - acc: 0.9978 - recall: 0.9978 - val_loss: 0.4834 - val_acc: 0.8133 - val_recall: 0.8067\n",
      "Epoch 16/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0393 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.4493 - val_acc: 0.8467 - val_recall: 0.8467\n",
      "Epoch 17/20\n",
      "450/450 [==============================] - 2s 5ms/step - loss: 0.0329 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.4650 - val_acc: 0.8400 - val_recall: 0.8333\n",
      "Epoch 18/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0284 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.4738 - val_acc: 0.8400 - val_recall: 0.8400\n",
      "Epoch 19/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0244 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.5540 - val_acc: 0.8200 - val_recall: 0.8200\n",
      "Epoch 20/20\n",
      "450/450 [==============================] - 2s 5ms/step - loss: 0.0216 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.5571 - val_acc: 0.8400 - val_recall: 0.8400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25426a77400>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',recall])\n",
    "model.fit(train_x, train_y, epochs=20, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2 0 2 2 1 0 2 0 2 0 2 1 0 0 2 0 2 1 1 0 0 0 1 0 2 0 0 1 2 2 2 1 1 1 2\n",
      " 2 2 2 0 2 1 1 2 0 2 0 0 0 0 2 1 1 0 2 2 1 0 2 1 0 2 0 1 0 0 1 0 0 1 2 0 2\n",
      " 0 0 0 0 0 2 2 1 0 0 1 0 0 2 2 0 2 1 1 0 1 0 1 2 2 1 0 0 2 0 2 0 2 0 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 1 0 2 1 0 2 0 1 2 0 1 2 0 0 1 0 2 1 0 2 2 2 0 0 2 0 1 1\n",
      " 0 2]\n"
     ]
    }
   ],
   "source": [
    "test_prob = model.predict(test_x)\n",
    "test_prob[5]\n",
    "test_classes = model.predict_classes(test_x)\n",
    "test_classes.shape\n",
    "print(test_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now using Optimizer='rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back=7##here i am giving to 4 words at a time stamp\n",
    "model = Sequential() # Call Sequential to initialize a network\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index), \n",
    "                    input_length=100, \n",
    "                    output_dim=150)) # Add an embedding layer which represents each unique token as a vector\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(12,input_shape=(1, look_back), return_sequences=False)) # Add an LSTM layer\n",
    "\n",
    "\n",
    "model.add(Dense(3, activation='softmax')) # Add an ouput layer. Since classification, 3 nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450 samples, validate on 150 samples\n",
      "Epoch 1/20\n",
      "450/450 [==============================] - 6s 12ms/step - loss: 1.0929 - acc: 0.4067 - recall: 0.0000e+00 - val_loss: 1.0899 - val_acc: 0.4333 - val_recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 1.0541 - acc: 0.6978 - recall: 0.0000e+00 - val_loss: 1.0696 - val_acc: 0.5133 - val_recall: 0.0000e+00\n",
      "Epoch 3/20\n",
      "450/450 [==============================] - 2s 5ms/step - loss: 0.9779 - acc: 0.7933 - recall: 0.0044 - val_loss: 1.0125 - val_acc: 0.5200 - val_recall: 0.0067\n",
      "Epoch 4/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.7671 - acc: 0.8067 - recall: 0.4533 - val_loss: 0.8172 - val_acc: 0.6667 - val_recall: 0.4933\n",
      "Epoch 5/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.5747 - acc: 0.8778 - recall: 0.6622 - val_loss: 0.6852 - val_acc: 0.7800 - val_recall: 0.5933\n",
      "Epoch 6/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.4368 - acc: 0.9644 - recall: 0.7978 - val_loss: 0.6713 - val_acc: 0.7467 - val_recall: 0.6133\n",
      "Epoch 7/20\n",
      "450/450 [==============================] - 2s 5ms/step - loss: 0.3401 - acc: 0.9822 - recall: 0.9489 - val_loss: 0.5982 - val_acc: 0.7933 - val_recall: 0.7000\n",
      "Epoch 8/20\n",
      "450/450 [==============================] - 2s 5ms/step - loss: 0.2616 - acc: 0.9956 - recall: 0.9911 - val_loss: 0.4352 - val_acc: 0.9133 - val_recall: 0.8733\n",
      "Epoch 9/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.2072 - acc: 0.9911 - recall: 0.9911 - val_loss: 0.4085 - val_acc: 0.9000 - val_recall: 0.8600\n",
      "Epoch 10/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.1585 - acc: 0.9978 - recall: 0.9978 - val_loss: 0.4610 - val_acc: 0.8400 - val_recall: 0.8400\n",
      "Epoch 11/20\n",
      "450/450 [==============================] - 2s 5ms/step - loss: 0.1314 - acc: 0.9978 - recall: 0.9978 - val_loss: 0.3847 - val_acc: 0.8867 - val_recall: 0.8800\n",
      "Epoch 12/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.1082 - acc: 0.9956 - recall: 0.9933 - val_loss: 0.3676 - val_acc: 0.8933 - val_recall: 0.8867\n",
      "Epoch 13/20\n",
      "450/450 [==============================] - 2s 5ms/step - loss: 0.0872 - acc: 0.9956 - recall: 0.9956 - val_loss: 0.3755 - val_acc: 0.8933 - val_recall: 0.8867\n",
      "Epoch 14/20\n",
      "450/450 [==============================] - 2s 5ms/step - loss: 0.0750 - acc: 0.9956 - recall: 0.9933 - val_loss: 0.4069 - val_acc: 0.8733 - val_recall: 0.8733\n",
      "Epoch 15/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0538 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.3750 - val_acc: 0.8867 - val_recall: 0.8867\n",
      "Epoch 16/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0419 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.5104 - val_acc: 0.8467 - val_recall: 0.8467\n",
      "Epoch 17/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0457 - acc: 0.9933 - recall: 0.9933 - val_loss: 0.5200 - val_acc: 0.8533 - val_recall: 0.8533\n",
      "Epoch 18/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0822 - acc: 0.9867 - recall: 0.9867 - val_loss: 0.4788 - val_acc: 0.8600 - val_recall: 0.8600\n",
      "Epoch 19/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0377 - acc: 0.9956 - recall: 0.9956 - val_loss: 0.4766 - val_acc: 0.8600 - val_recall: 0.8600\n",
      "Epoch 20/20\n",
      "450/450 [==============================] - 2s 5ms/step - loss: 0.0219 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.4962 - val_acc: 0.8600 - val_recall: 0.8600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2542c55e828>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy',recall])\n",
    "model.fit(train_x, train_y, epochs=20, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back=7##here i am giving to 4 words at a time stamp\n",
    "model = Sequential() # Call Sequential to initialize a network\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index), \n",
    "                    input_length=100, \n",
    "                    output_dim=150)) # Add an embedding layer which represents each unique token as a vector\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(12,input_shape=(1, look_back), return_sequences=False)) # Add an LSTM layer\n",
    "\n",
    "\n",
    "model.add(Dense(3, activation='softmax')) # Add an ouput layer. Since classification, 3 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################ADAM hyper tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Hypertuning The Adam\n",
    "adam = optimizers.Adam(lr=0.01, decay=1e-6, beta_1=.8 ,beta_2=.85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450 samples, validate on 150 samples\n",
      "Epoch 1/20\n",
      "450/450 [==============================] - 7s 15ms/step - loss: 0.0428 - acc: 0.9978 - recall: 0.9978 - val_loss: 0.6059 - val_acc: 0.7400 - val_recall: 0.7400\n",
      "Epoch 2/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0334 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.6511 - val_acc: 0.7333 - val_recall: 0.7333\n",
      "Epoch 3/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0247 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.6396 - val_acc: 0.7533 - val_recall: 0.7533\n",
      "Epoch 4/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0335 - acc: 0.9956 - recall: 0.9956 - val_loss: 0.5885 - val_acc: 0.7800 - val_recall: 0.7800\n",
      "Epoch 5/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0462 - acc: 0.9889 - recall: 0.9889 - val_loss: 0.6471 - val_acc: 0.7667 - val_recall: 0.7600\n",
      "Epoch 6/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0155 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.6590 - val_acc: 0.7733 - val_recall: 0.7733\n",
      "Epoch 7/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0134 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.6715 - val_acc: 0.7733 - val_recall: 0.7667\n",
      "Epoch 8/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0116 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.6566 - val_acc: 0.7733 - val_recall: 0.7733\n",
      "Epoch 9/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0100 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.6789 - val_acc: 0.7733 - val_recall: 0.7667\n",
      "Epoch 10/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0088 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.6793 - val_acc: 0.7800 - val_recall: 0.7733\n",
      "Epoch 11/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0082 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.6821 - val_acc: 0.7867 - val_recall: 0.7867\n",
      "Epoch 12/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0071 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.7310 - val_acc: 0.7733 - val_recall: 0.7733\n",
      "Epoch 13/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0064 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.7506 - val_acc: 0.7733 - val_recall: 0.7733\n",
      "Epoch 14/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0059 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.7254 - val_acc: 0.7800 - val_recall: 0.7800\n",
      "Epoch 15/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0054 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.6720 - val_acc: 0.8067 - val_recall: 0.8067\n",
      "Epoch 16/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0049 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.6703 - val_acc: 0.8000 - val_recall: 0.8000\n",
      "Epoch 17/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0045 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.6834 - val_acc: 0.8000 - val_recall: 0.8000\n",
      "Epoch 18/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0041 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.6953 - val_acc: 0.8067 - val_recall: 0.8067\n",
      "Epoch 19/20\n",
      "450/450 [==============================] - 2s 5ms/step - loss: 0.0038 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.7382 - val_acc: 0.7933 - val_recall: 0.7933\n",
      "Epoch 20/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0035 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.6580 - val_acc: 0.8133 - val_recall: 0.8133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25434ea6320>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',recall])\n",
    "model.fit(train_x, train_y, epochs=20, validation_split=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##now I am using normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building an LSTM model\n",
    "look_back=7##here i am giving to 4 words at a time stamp\n",
    "\n",
    "model = Sequential() # Call Sequential to initialize a network\n",
    "\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index), \n",
    "                    input_length=100, \n",
    "                    output_dim=100)) # Add an embedding layer which represents each unique token as a vector\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(15,input_shape=(1, look_back), return_sequences=False)) # Add an LSTM layer\n",
    "\n",
    "model.add(Dense(3, activation='softmax')) # Add an ouput layer. Since classification, 3 nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450 samples, validate on 150 samples\n",
      "Epoch 1/20\n",
      "450/450 [==============================] - 5s 12ms/step - loss: 1.1099 - acc: 0.3644 - recall: 0.0267 - val_loss: 1.0568 - val_acc: 0.4067 - val_recall: 0.0200\n",
      "Epoch 2/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.9667 - acc: 0.5844 - recall: 0.1289 - val_loss: 1.0142 - val_acc: 0.4667 - val_recall: 0.0800\n",
      "Epoch 3/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.8539 - acc: 0.7022 - recall: 0.2889 - val_loss: 0.9821 - val_acc: 0.4933 - val_recall: 0.1667\n",
      "Epoch 4/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.7360 - acc: 0.8200 - recall: 0.4733 - val_loss: 0.9560 - val_acc: 0.5333 - val_recall: 0.2333\n",
      "Epoch 5/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.5966 - acc: 0.8867 - recall: 0.7089 - val_loss: 0.9293 - val_acc: 0.5267 - val_recall: 0.2667\n",
      "Epoch 6/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.4726 - acc: 0.9333 - recall: 0.8289 - val_loss: 0.9062 - val_acc: 0.5600 - val_recall: 0.3333\n",
      "Epoch 7/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.3400 - acc: 0.9644 - recall: 0.9178 - val_loss: 0.8858 - val_acc: 0.5533 - val_recall: 0.3667\n",
      "Epoch 8/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.2294 - acc: 0.9844 - recall: 0.9711 - val_loss: 0.8792 - val_acc: 0.5600 - val_recall: 0.4200\n",
      "Epoch 9/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.1471 - acc: 0.9911 - recall: 0.9867 - val_loss: 0.8776 - val_acc: 0.5667 - val_recall: 0.4400\n",
      "Epoch 10/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.1045 - acc: 0.9911 - recall: 0.9867 - val_loss: 0.8709 - val_acc: 0.5800 - val_recall: 0.4400\n",
      "Epoch 11/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0709 - acc: 0.9956 - recall: 0.9911 - val_loss: 0.8837 - val_acc: 0.5733 - val_recall: 0.4333\n",
      "Epoch 12/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0442 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.8929 - val_acc: 0.5867 - val_recall: 0.4533\n",
      "Epoch 13/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0326 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.9035 - val_acc: 0.5867 - val_recall: 0.4667\n",
      "Epoch 14/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0235 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.9120 - val_acc: 0.5867 - val_recall: 0.4800\n",
      "Epoch 15/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0195 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.9187 - val_acc: 0.6000 - val_recall: 0.5000\n",
      "Epoch 16/20\n",
      "450/450 [==============================] - 2s 5ms/step - loss: 0.0165 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.9359 - val_acc: 0.5933 - val_recall: 0.5267\n",
      "Epoch 17/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0134 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.9458 - val_acc: 0.5800 - val_recall: 0.5200\n",
      "Epoch 18/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0121 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.9578 - val_acc: 0.5867 - val_recall: 0.5067\n",
      "Epoch 19/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0098 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.9641 - val_acc: 0.5867 - val_recall: 0.5200\n",
      "Epoch 20/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0088 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.9715 - val_acc: 0.5933 - val_recall: 0.5200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2541d013da0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',recall])\n",
    "model.fit(train_x, train_y, epochs=20, validation_split=0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can see that when I use normalzation accuracy dicreases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450 samples, validate on 150 samples\n",
      "Epoch 1/20\n",
      "450/450 [==============================] - 6s 13ms/step - loss: 1.0952 - acc: 0.3778 - recall: 0.0000e+00 - val_loss: 1.0877 - val_acc: 0.5067 - val_recall: 0.0000e+00\n",
      "Epoch 2/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 1.0621 - acc: 0.6733 - recall: 0.0000e+00 - val_loss: 1.0757 - val_acc: 0.5600 - val_recall: 0.0000e+00\n",
      "Epoch 3/20\n",
      "450/450 [==============================] - 2s 5ms/step - loss: 1.0125 - acc: 0.7844 - recall: 0.0000e+00 - val_loss: 1.0553 - val_acc: 0.5200 - val_recall: 0.0000e+00\n",
      "Epoch 4/20\n",
      "450/450 [==============================] - 2s 5ms/step - loss: 0.9177 - acc: 0.7733 - recall: 0.1111 - val_loss: 1.0293 - val_acc: 0.4467 - val_recall: 0.1533\n",
      "Epoch 5/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.7306 - acc: 0.8133 - recall: 0.3644 - val_loss: 1.0367 - val_acc: 0.5267 - val_recall: 0.3200\n",
      "Epoch 6/20\n",
      "450/450 [==============================] - 2s 5ms/step - loss: 0.5605 - acc: 0.9467 - recall: 0.5511 - val_loss: 0.8320 - val_acc: 0.7333 - val_recall: 0.3067\n",
      "Epoch 7/20\n",
      "450/450 [==============================] - 2s 5ms/step - loss: 0.4404 - acc: 0.9822 - recall: 0.7822 - val_loss: 0.7025 - val_acc: 0.7733 - val_recall: 0.5267\n",
      "Epoch 8/20\n",
      "450/450 [==============================] - 2s 6ms/step - loss: 0.3635 - acc: 0.9911 - recall: 0.9156 - val_loss: 0.7643 - val_acc: 0.7600 - val_recall: 0.6067\n",
      "Epoch 9/20\n",
      "450/450 [==============================] - 3s 6ms/step - loss: 0.3080 - acc: 0.9911 - recall: 0.9756 - val_loss: 0.6470 - val_acc: 0.7867 - val_recall: 0.6800\n",
      "Epoch 10/20\n",
      "450/450 [==============================] - 2s 5ms/step - loss: 0.2250 - acc: 0.9978 - recall: 0.9933 - val_loss: 0.6129 - val_acc: 0.8400 - val_recall: 0.7933\n",
      "Epoch 11/20\n",
      "450/450 [==============================] - 2s 5ms/step - loss: 0.1965 - acc: 0.9933 - recall: 0.9933 - val_loss: 0.5985 - val_acc: 0.8400 - val_recall: 0.8067\n",
      "Epoch 12/20\n",
      "450/450 [==============================] - 2s 5ms/step - loss: 0.1537 - acc: 0.9933 - recall: 0.9933 - val_loss: 0.5625 - val_acc: 0.8067 - val_recall: 0.7867\n",
      "Epoch 13/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.1171 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.5113 - val_acc: 0.8667 - val_recall: 0.8533\n",
      "Epoch 14/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.1014 - acc: 0.9978 - recall: 0.9978 - val_loss: 0.5268 - val_acc: 0.8467 - val_recall: 0.8400\n",
      "Epoch 15/20\n",
      "450/450 [==============================] - 2s 4ms/step - loss: 0.0859 - acc: 0.9978 - recall: 0.9978 - val_loss: 0.5226 - val_acc: 0.8533 - val_recall: 0.8533\n",
      "Epoch 16/20\n",
      "450/450 [==============================] - 2s 5ms/step - loss: 0.0660 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.5529 - val_acc: 0.8333 - val_recall: 0.8333\n",
      "Epoch 17/20\n",
      "450/450 [==============================] - 2s 5ms/step - loss: 0.0577 - acc: 0.9933 - recall: 0.9933 - val_loss: 0.7535 - val_acc: 0.8200 - val_recall: 0.8200\n",
      "Epoch 18/20\n",
      "450/450 [==============================] - 2s 5ms/step - loss: 0.0427 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.6909 - val_acc: 0.7933 - val_recall: 0.7933\n",
      "Epoch 19/20\n",
      "450/450 [==============================] - 2s 5ms/step - loss: 0.0339 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.6620 - val_acc: 0.8000 - val_recall: 0.8000\n",
      "Epoch 20/20\n",
      "450/450 [==============================] - 2s 5ms/step - loss: 0.0258 - acc: 1.0000 - recall: 1.0000 - val_loss: 0.8523 - val_acc: 0.7800 - val_recall: 0.7800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x254396acf60>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_back=7##here i am giving to 4 words at a time stamp\n",
    "model = Sequential() # Call Sequential to initialize a network\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index), \n",
    "                    input_length=100, \n",
    "                    output_dim=150)) # Add an embedding layer which represents each unique token as a vector\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(12,input_shape=(1, look_back), return_sequences=False)) # Add an LSTM layer\n",
    "\n",
    "\n",
    "model.add(Dense(3, activation='softmax')) # Add an ouput layer. Since classification, 3 nodes\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy',recall])\n",
    "model.fit(train_x, train_y, epochs=20, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.embeddings.Embedding at 0x25432694048>,\n",
       " <keras.layers.core.Dropout at 0x25432694128>,\n",
       " <keras.layers.recurrent.LSTM at 0x254326942e8>,\n",
       " <keras.layers.core.Dense at 0x254326943c8>]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "1\n",
      "(5, 100, 150)\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K \n",
    "# Create a user defined function in keras, where we mention the input and output\n",
    "# This function returns a list\n",
    "eo = K.function([model.layers[0].input],\n",
    "                  [model.layers[0].output])\n",
    "\n",
    "out = eo([train_x[0:5]]) \n",
    "print(type(out))\n",
    "print(len(out))\n",
    "print(out[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.00406499, -0.03752612,  0.02748853, ...,  0.03032188,\n",
       "         -0.01770221, -0.0141458 ],\n",
       "        [-0.01464888, -0.00491512,  0.07457757, ...,  0.04264292,\n",
       "          0.01147406, -0.02281868],\n",
       "        [ 0.05290215, -0.03380545,  0.06093838, ...,  0.06677757,\n",
       "          0.00228953,  0.0051606 ],\n",
       "        ...,\n",
       "        [ 0.02954119,  0.02109104,  0.0712281 , ...,  0.01830088,\n",
       "          0.05430136,  0.02652512],\n",
       "        [ 0.01524809,  0.01126303,  0.05917569, ...,  0.00049231,\n",
       "          0.02038612, -0.08786622],\n",
       "        [ 0.08145417,  0.03187617,  0.07208585, ..., -0.02405494,\n",
       "          0.0391557 ,  0.00609524]],\n",
       "\n",
       "       [[-0.01036389,  0.02318658,  0.04365764, ...,  0.0020917 ,\n",
       "          0.00108375,  0.02792811],\n",
       "        [-0.01303279, -0.00939311,  0.03490455, ...,  0.00327179,\n",
       "          0.03049978, -0.08557211],\n",
       "        [ 0.01992651,  0.00969533, -0.02897304, ..., -0.03797258,\n",
       "          0.0065158 ,  0.02314282],\n",
       "        ...,\n",
       "        [-0.00243311,  0.0309361 ,  0.01980856, ...,  0.01258015,\n",
       "          0.06186053, -0.05660142],\n",
       "        [-0.00581172,  0.02066685,  0.04223021, ...,  0.05032176,\n",
       "          0.0183273 ,  0.03150553],\n",
       "        [ 0.02000713, -0.01520593,  0.09278275, ..., -0.01182706,\n",
       "          0.07224511, -0.05713043]],\n",
       "\n",
       "       [[ 0.02316377,  0.02754257,  0.08396535, ...,  0.03404936,\n",
       "          0.00514949, -0.10394452],\n",
       "        [ 0.04503096, -0.05047297,  0.01943131, ..., -0.01785052,\n",
       "          0.02476406, -0.04581705],\n",
       "        [ 0.02967972, -0.03420129,  0.06908327, ..., -0.02739459,\n",
       "          0.0688609 , -0.04019139],\n",
       "        ...,\n",
       "        [ 0.0056653 ,  0.04545036,  0.00081145, ...,  0.00706057,\n",
       "          0.03373775,  0.01967732],\n",
       "        [ 0.01524809,  0.01126303,  0.05917569, ...,  0.00049231,\n",
       "          0.02038612, -0.08786622],\n",
       "        [ 0.05146986,  0.07193818,  0.03485402, ...,  0.00925149,\n",
       "          0.05180784, -0.02036842]],\n",
       "\n",
       "       [[-0.03527465, -0.06361337,  0.03782192, ...,  0.01442758,\n",
       "         -0.01485837,  0.00605646],\n",
       "        [ 0.02954119,  0.02109104,  0.0712281 , ...,  0.01830088,\n",
       "          0.05430136,  0.02652512],\n",
       "        [-0.02551029, -0.02106835,  0.06421632, ...,  0.04220266,\n",
       "          0.0508119 , -0.00269372],\n",
       "        ...,\n",
       "        [-0.05189431,  0.00121709,  0.03147417, ..., -0.05376745,\n",
       "          0.0210996 , -0.09624857],\n",
       "        [-0.04448683, -0.07104469,  0.01588522, ..., -0.08540468,\n",
       "          0.03503441, -0.04079797],\n",
       "        [-0.04532707, -0.07797636,  0.01307064, ..., -0.03396371,\n",
       "          0.04458509, -0.03677515]],\n",
       "\n",
       "       [[ 0.00466943, -0.05460126,  0.03096475, ..., -0.01320562,\n",
       "          0.03734962,  0.03315419],\n",
       "        [ 0.02162804, -0.07615804,  0.00427125, ..., -0.00940482,\n",
       "          0.01522251, -0.01362553],\n",
       "        [ 0.02967972, -0.03420129,  0.06908327, ..., -0.02739459,\n",
       "          0.0688609 , -0.04019139],\n",
       "        ...,\n",
       "        [ 0.06850897,  0.0382887 ,  0.02479284, ..., -0.00795446,\n",
       "          0.05032137, -0.01726777],\n",
       "        [ 0.02954119,  0.02109104,  0.0712281 , ...,  0.01830088,\n",
       "          0.05430136,  0.02652512],\n",
       "        [ 0.04625111, -0.02779655,  0.00268816, ...,  0.01541578,\n",
       "         -0.03483092,  0.00344858]]], dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer = model.get_layer('block5_pool').output ## here i am using \"fc2 layer \n",
    "x = Flatten(name='flatten')(last_layer)\n",
    "x = Dense(32, activation='relu', name='fc1')(x)\n",
    "\n",
    "out = Dense(num_classes, activation='softmax', name='output')(x)\n",
    "custom_vgg_model3 = Model(image_input, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "look_back=7##here i am giving to 4 words at a time stamp\n",
    "model = Sequential() # Call Sequential to initialize a network\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index), \n",
    "                    input_length=100, \n",
    "                    output_dim=150)) # Add an embedding layer which represents each unique token as a vector\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(12,input_shape=(1, look_back), return_sequences=False)) # Add an LSTM layer\n",
    "\n",
    "\n",
    "model.add(Dense(3, activation='softmax')) # Add an ouput layer. Since classification, 3 nodes\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy',recall])\n",
    "model.fit(train_x, train_y, epochs=20, validation_split=0.25)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
